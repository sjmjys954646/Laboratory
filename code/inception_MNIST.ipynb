{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inception_presentation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaJU-Ubk_28s"
      },
      "source": [
        "import os\n",
        "\n",
        "# os.environ[\"KERAS_BACKEND\"] = 'plaidml.keras.backend'\n",
        "\n",
        "import keras.backend as K\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPool2D, Dropout, Dense, Input, concatenate, AveragePooling2D, Flatten, Layer\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import math\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loading dataset and Performing some preprocessing steps.\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "# The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly\n",
        "# used to train machine learning and computer vision algorithms.\n",
        "def load_cifar10_data(img_rows, img_cols):\n",
        "\n",
        "    \"\"\"\n",
        "    Cifar Image를 다운로드 받아서\n",
        "    이미지를 training과 valid로 나누고\n",
        "    Preprocessing을 해 준다.\n",
        "    :param img_rows: 리사이징 이미지 크기 (Row)\n",
        "    :param img_cols: 리사이징 이미지 크기 (Col)\n",
        "    :return: normalized된 train과 valid 이미지 numpy array\n",
        "    \"\"\"\n",
        "\n",
        "    # x는 feature y는 label\n",
        "    # load cifar-10 training and validation sets\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "    print(x_train.shape)\n",
        "    x_train = x_train[0:6000, :, :]\n",
        "    y_train = y_train[0:6000]\n",
        "    print(x_train.shape)\n",
        "    x_test = x_test[6000:8000,  :, :]\n",
        "    y_test = y_test[6000:8000]\n",
        "\n",
        "    # resize training images\n",
        "    x_train = np.array([cv2.resize(img, (img_rows, img_cols)) for img in x_train[:, :, :]])\n",
        "    x_test = np.array([cv2.resize(img, (img_rows, img_cols)) for img in x_test[:, :, :]])\n",
        "    print(x_train.shape)\n",
        "    # transform targets to keras compatible format\n",
        "    y_train = np_utils.to_categorical(y_train, num_classes)\n",
        "    y_test = np_utils.to_categorical(y_test, num_classes)\n",
        "    # One-hot 인코딩 해주는 함수 10진수->2진 binary\n",
        "\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    # convert from integers to floats\n",
        "\n",
        "    # preprocess data (영상이미지라서 255.0으로 나눠서 normalize한다) normalize to range 0-1\n",
        "    x_train = x_train / 255.0\n",
        "    x_test = x_test / 255.0\n",
        "\n",
        "    return x_train, y_train, x_test, y_test\n",
        "\n",
        "\n",
        "# define inception v1 architecture\n",
        "def inception_module(x, filters_1x1, filters_3x3_reduce, filters_3x3, filters_5x5_reduce, filters_5x5,\n",
        "                     filters_pool_proj, name=None, bias_init='zeros'):\n",
        "    \"\"\"\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\n",
        "    tf.keras.layers.Conv2D(\n",
        "    filters, kernel_size, strides=(1, 1), padding='valid',\n",
        "    data_format=None, dilation_rate=(1, 1), groups=1, activation=None,\n",
        "    use_bias=True, kernel_initializer='glorot_uniform',\n",
        "    bias_initializer='zeros', kernel_regularizer=None,\n",
        "    bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
        "    bias_constraint=None, **kwargs\n",
        "    )\n",
        "\n",
        "    kernel_initializer :\n",
        "    Initializer for the kernel weights matrix (see keras.initializers). Defaults to 'glorot_uniform'.\n",
        "    bias_initializer :\n",
        "    Initializer for the bias vector (see keras.initializers). Defaults to 'zeros'.\n",
        "\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D\n",
        "    tf.keras.layers.MaxPool2D(\n",
        "    pool_size=(2, 2), strides=None, padding='valid', data_format=None,\n",
        "    **kwargs\n",
        "\n",
        "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/concatenate\n",
        "    tf.keras.layers.concatenate(\n",
        "    inputs, axis=-1, **kwargs\n",
        "    axis? https://supermemi.tistory.com/11\n",
        "    \"\"\"\n",
        "\n",
        "    conv_1x1 = Conv2D(filters_1x1, (1, 1), padding='same', activation='relu', bias_initializer=bias_init)(x)\n",
        "\n",
        "    conv_3x3_reduce = Conv2D(filters_3x3_reduce, (1, 1), padding='same', activation='relu', bias_initializer=bias_init)(x)\n",
        "\n",
        "    conv_3x3 = Conv2D(filters_3x3, (3, 3), padding='same', activation='relu', bias_initializer=bias_init)(conv_3x3_reduce)\n",
        "\n",
        "    conv_5x5_reduce = Conv2D(filters_5x5_reduce, (1, 1), padding='same', activation='relu', bias_initializer=bias_init)(x)\n",
        "\n",
        "    conv_5x5 = Conv2D(filters_5x5, (5, 5), padding='same', activation='relu', bias_initializer=bias_init)(conv_5x5_reduce)\n",
        "\n",
        "    max_pool = MaxPool2D((3, 3), strides=(1, 1), padding='same')(x)\n",
        "\n",
        "    pool_proj = Conv2D(filters_pool_proj, (1, 1), padding='same', activation='relu', bias_initializer=bias_init)(max_pool)\n",
        "\n",
        "    output = concatenate([conv_1x1, conv_3x3, conv_5x5, pool_proj], axis=3, name=name)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def decay(epoch, steps=100):\n",
        "    initial_lrate = 0.01\n",
        "    drop = 0.96\n",
        "    epoch_drop = 8\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epoch_drop))\n",
        "    return lrate\n",
        "#https://machinelearningmastery.com/using-learning-rate-schedules-deep-learning-models-python-keras/\n",
        "#더 낮은 loss로 수렴하기 위해서 학습률을 감소시키는 방법을 사용\n",
        "\n",
        "\"\"\"\n",
        "class LRN2D(Layer):\n",
        "     This code is adapted from pylearn2.\n",
        "     License at: https://github.com/lisa-lab/pylearn2/blob/master/LICENSE.txt\n",
        "    \n",
        "\n",
        "    def __init__(self, alpha=0.0001,k=1,beta=0.75,n=3, name=None):\n",
        "        if n % 2 == 0:\n",
        "            raise NotImplementedError(\"LRN2D only works with odd n. n provided: \" + str(n))\n",
        "        super(LRN2D, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.k = k\n",
        "        self.beta = beta\n",
        "        self.n = n\n",
        "        self.name = name\n",
        "\n",
        "    def get_output(self, train):\n",
        "        X = self.get_input(train)\n",
        "        return tf.nn.lrn(X)\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\"name\": self.__class__.__name__,\n",
        "                \"alpha\": self.alpha,\n",
        "                \"k\": self.k,\n",
        "                \"beta\": self.beta,\n",
        "                \"n\": self.n}\n",
        "\"\"\"\n",
        "#LRN 사용예시\n",
        "\n",
        "\n",
        "x_train, y_train, x_valid, y_valid = load_cifar10_data(224, 224)\n",
        "\n",
        "\"\"\"\n",
        "https://stackoverflow.com/questions/2480650/what-is-the-role-of-the-bias-in-neural-networks\n",
        "What is the role of the bias in neural networks?\n",
        "c\n",
        " which may be critical for successful learning.\n",
        "\n",
        " The main function of a bias is to provide every node with a trainable constant value (in addition to the normal inputs that the node recieves). \n",
        " You can achieve that with a single bias node with connections to N nodes, or with N bias nodes each with a single connection; the result should be the same.\n",
        "\"\"\"\n",
        "\n",
        "# 모든값이 특정 상수인 텐서를 생성하는 초기값 설정기\n",
        "bias_init = keras.initializers.Constant(value=0.2)\n",
        "\n",
        "input_layer = Input(shape=(224, 224, 1))\n",
        "\n",
        "# Layer 1\n",
        "x = Conv2D(64, (7, 7), padding='same', strides=(2, 2), activation='relu', name='conv_1_7x7/2',\n",
        "           bias_initializer=bias_init)(input_layer)\n",
        "\n",
        "x = MaxPool2D((3, 3), strides=(2, 2), name='max_pool_1_3x3/2', padding='same')(x)\n",
        "\n",
        "# https://taeguu.tistory.com/29\n",
        "# 실제 모델에서 LRN 사용 한 이유\n",
        "\n",
        "# Layer 2 \n",
        "x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', name='conv2/3x3_reduce',\n",
        "           bias_initializer=bias_init)(x)\n",
        "\n",
        "x = Conv2D(192, (3, 3), padding='same', strides=(1, 1), activation='relu', name='conv_2_3x3/1',\n",
        "           bias_initializer=bias_init)(x)\n",
        "\n",
        "x = MaxPool2D((3, 3), strides=(2, 2), name='max_pool_2_3x3/2', padding='same')(x)\n",
        "\n",
        "# Layer 3\n",
        "x = inception_module(x, 64, 96, 128, 16, 32, 32, name='inception_3a', bias_init=bias_init)\n",
        "x = inception_module(x, 128, 128, 192, 32, 96, 64, name='inception_3b', bias_init=bias_init)\n",
        "x = MaxPool2D((3, 3), strides=(2, 2), name='max_pool_3_3x3/2', padding='same')(x)\n",
        "\n",
        "# Layer 4\n",
        "x = inception_module(x, 192, 96, 208, 16, 48, 64, name='inception_4a')\n",
        "\n",
        "# Layer 4 - Auxiliary Learning 1\n",
        "x1 = AveragePooling2D((5, 5), strides=3, name='avg_pool_aux_1')(x)\n",
        "x1 = Conv2D(128, (1, 1), padding='same', activation='relu', name='conv_aux_1')(x1)\n",
        "x1 = Flatten()(x1)\n",
        "x1 = Dense(1024, activation='relu', name='dense_aux_1')(x1)\n",
        "x1 = Dropout(0.7)(x1)\n",
        "x1 = Dense(10, activation='softmax', name='aux_output_1')(x1)\n",
        "\n",
        "x = inception_module(x, 160, 112, 224, 24, 64, 64, name='inception_4b', bias_init=bias_init)\n",
        "x = inception_module(x, 128, 128, 256, 24, 64, 64, name='inception_4c', bias_init=bias_init)\n",
        "x = inception_module(x, 112, 144, 288, 32, 64, 64, name='inception_4d', bias_init=bias_init)\n",
        "\n",
        "# Layer 4 - Auxiliary Learning 2\n",
        "x2 = AveragePooling2D((5, 5), strides=3, name='avg_pool_aux_2')(x)\n",
        "x2 = Conv2D(128, (1, 1), padding='same', activation='relu', name='conv_aux_2')(x2)\n",
        "x2 = Flatten()(x2)\n",
        "x2 = Dense(1024, activation='relu', name='dense_aux_2')(x2)\n",
        "x2 = Dropout(0.7)(x2)\n",
        "x2 = Dense(10, activation='softmax', name='aux_output_2')(x2)\n",
        "\n",
        "x = inception_module(x, 256, 160, 320, 32, 128, 128, name='inception_4e', bias_init=bias_init)\n",
        "x = MaxPool2D((3, 3), strides=(2, 2), name='max_pool_4_3x3/2', padding='same')(x)\n",
        "\n",
        "# Layer 5\n",
        "x = inception_module(x, 256, 160, 320, 32, 128, 128, name='inception_5a', bias_init=bias_init)\n",
        "x = inception_module(x, 384, 192, 384, 48, 128, 128, name='inception_5b', bias_init=bias_init)\n",
        "x = AveragePooling2D((7, 7), strides=(1, 1), name='pool5/7x7_s2')(x)\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.4)(x)\n",
        "# regularization방식 중 하나\n",
        "x = Dense(10, activation='softmax', name='output')(x)\n",
        "\n",
        "# input -> output - model\n",
        "model = Model(input_layer, [x, x1, x2], name='inception_v1')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "epoch = 25\n",
        "initial_lrate = 0.01\n",
        "\n",
        "# 추출된 데이터에대해 error gradient를 계산, gradient descent 알고리즘을 적용하는 방법\n",
        "# https://mangkyu.tistory.com/62\n",
        "sgd = SGD(learning_rate=initial_lrate, momentum=0.9, nesterov=False)\n",
        "lr_sc = LearningRateScheduler(decay, verbose=1)\n",
        "# a function that takes an epoch index (integer, indexed from 0) and current learning rate (float) as inputs and returns a new learning rate as output (float).\n",
        "# 학습률을 조정하기 위한 함수\n",
        "\n",
        "model.compile(loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],\n",
        "              loss_weights=[1, 0.3, 0.3], optimizer=sgd, metrics=['accuracy'])\n",
        "# 분류 할때 label -> 배열 softmax_cross_entropy를 loss로 하고자 할 때\n",
        "# https://crazyj.tistory.com/153\n",
        "\n",
        "# 학습시키기\n",
        "# batch : 몇문항을 풀고 해답을맞추는지\n",
        "# epoch : 같은문제를 몇번 풀어볼지 (overfitting)\n",
        "history = model.fit(x_train, [y_train, y_train, y_train], validation_data=(x_valid, [y_valid, y_valid, y_valid]), epochs=epoch, batch_size=20, callbacks=[lr_sc])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjjyhJa6AIX6"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvNEtn6hAJ6l"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['output_accuracy'])\n",
        "plt.plot(history.history['aux_output_1_accuracy'])\n",
        "plt.plot(history.history['aux_output_2_accuracy'])\n",
        "plt.plot(history.history['val_output_accuracy'])\n",
        "plt.plot(history.history['val_aux_output_1_accuracy'])\n",
        "plt.plot(history.history['val_aux_output_2_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['output_accuracy', 'aux_output_1_accuracy','aux_output_2_accuracy','val_output_accuracy','val_aux_output_1_accuracy','val_aux_output_2_accuracy'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['output_loss'])\n",
        "plt.plot(history.history['aux_output_1_loss'])\n",
        "plt.plot(history.history['aux_output_2_loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.plot(history.history['val_output_loss'])\n",
        "plt.plot(history.history['val_aux_output_1_loss'])\n",
        "plt.plot(history.history['val_aux_output_2_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['loss', 'output_loss','aux_output_1_loss','aux_output_2_loss','val_lossval_loss','val_output_loss','val_aux_output_1_loss','val_aux_output_2_loss'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}